labs(title = "Rozkład RI wg skupień PAM",
x = "Skupienie", y = "RI") +
theme_minimal()
plot_na <- ggplot(dane_with_clusters, aes(x = factor(PAM_Cluster), y = Na, fill = factor(PAM_Cluster))) +
geom_boxplot() +
labs(title = "Rozkład sodu (Na) wg skupień PAM",
x = "Skupienie", y = "Na") +
theme_minimal()
plot_ri + plot_na
library(knitr)
cechy <- names(dane)[names(dane) != "Type"]
medoids <- dane[rownames(dane) %in% rownames(pam_result$medoids), cechy, drop = FALSE]
medoid_stats <- rbind(
"Medoid 1" = as.numeric(medoids[1, cechy]),
"Średnia 1" = colMeans(dane[pam_clusters == 1, cechy]),
"Medoid 2" = as.numeric(medoids[2, cechy]),
"Średnia 2" = colMeans(dane[pam_clusters == 2, cechy])
)
medoid_stats_rounded <- round(medoid_stats, 2)
kable(medoid_stats_rounded,
caption = "Porownanie medoidow i srednich",
align = "c")
Glass.agnes.avg <- agnes(x = Glass.MacNiepodob, diss = TRUE, method = "average")
Glass.agnes.single <- agnes(x = Glass.MacNiepodob, diss = TRUE, method = "single")
Glass.agnes.complete <- agnes(x = Glass.MacNiepodob, diss = TRUE, method = "complete")
dend_avg <- fviz_dend(Glass.agnes.avg, k = 6, cex = 0.5, main = "AGNES: average linkage (Glass)")
dend_single <- fviz_dend(Glass.agnes.single, k = 6, cex = 0.5, main = "AGNES: single linkage (Glass)")
dend_complete <- fviz_dend(Glass.agnes.complete, k = 6, cex = 0.5, main = "AGNES: complete linkage (Glass)")
# Połączenie wykresów w jeden rząd
dend_avg + dend_single + dend_complete
agnes_results <- list(
average = Glass.agnes.avg,
single = Glass.agnes.single,
complete = Glass.agnes.complete
)
# Funkcja do obliczania współczynnika aglomeracji
coeff <- sapply(agnes_results, function(x) x$ac)
najw = coeff[3]
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
fviz_silhouette(sil_glass) +
ggtitle("Wykres silhouette dla metody average linkage (K=6)")
fviz_pca_biplot(Glass.PCA,
col.ind = as.factor(Glass.agnes.k6),
geom.ind = "point",
palette = "jco",
addEllipses = TRUE,
ellipse.type = "convex",
col.var = "black",
legend.title = "Skupienia AGNES",
title = "Biplot: Skupienia i cechy chemiczne (AGNES)") +
theme_minimal()
Glass.agnes.avg <- agnes(x = Glass.MacNiepodob, diss = TRUE, method = "average")
Glass.agnes.single <- agnes(x = Glass.MacNiepodob, diss = TRUE, method = "single")
Glass.agnes.complete <- agnes(x = Glass.MacNiepodob, diss = TRUE, method = "complete")
dend_avg <- fviz_dend(Glass.agnes.avg, k = 6, cex = 0.5, main = "AGNES: average linkage (Glass)")
dend_single <- fviz_dend(Glass.agnes.single, k = 6, cex = 0.5, main = "AGNES: single linkage (Glass)")
dend_complete <- fviz_dend(Glass.agnes.complete, k = 6, cex = 0.5, main = "AGNES: complete linkage (Glass)")
dend_avg + dend_single + dend_complete
agnes_results <- list(
average = Glass.agnes.avg,
single = Glass.agnes.single,
complete = Glass.agnes.complete
)
coeff <- sapply(agnes_results, function(x) x$ac)
najw = coeff[3]
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
plot_sil <- fviz_silhouette(sil_glass)
print(plot_sil + ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
plot_sil <- fviz_silhouette(sil_glass)
print(plot_sil + ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
plot_sil <- fviz_silhouette(sil_glass)
plot_sil + ggtitle("Wykres silhouette dla metody complete linkage (K=6)")
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
invisible({
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
plot_sil <- fviz_silhouette(sil_glass)
print(plot_sil + ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
})
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
invisible({
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
plot_sil <- fviz_silhouette(sil_glass)
print(plot_sil + ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
})
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
plot_sil <- fviz_silhouette(sil_glass)
plot_sil$data <- NULL
print(plot_sil + ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
plot_sil <- fviz_silhouette(sil_glass)
str(plot_sil)
print(plot_sil + ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
plot_sil <- fviz_silhouette(sil_glass)
plot_sil$data <- NULL
print(plot_sil + ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
plot_sil <- fviz_silhouette(sil_glass)
plot_sil$data <- NULL
str(plot_sil)
print(plot_sil + ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
# Tylko wykres — bez printowania pod spodem zbędnej tabeli:
plot_sil <- fviz_silhouette(sil_glass)
# Render wykresu
print(plot_sil + ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
# Tylko wykres — bez printowania pod spodem zbędnej tabeli:
sil_plot <- fviz_silhouette(sil_glass)
# Wyświetl tylko wykres — bez dataframe'a
print(sil_plot$plot + ggtitle("Wykres silhouette dla metody complete linkage (K=6)")
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
# Tylko wykres — bez printowania pod spodem zbędnej tabeli:
sil_plot <- fviz_silhouette(sil_glass)
# Wyświetl tylko wykres — bez dataframe'a
print(sil_plot + ggtitle("Wykres silhouette dla metody complete linkage (K=6)")
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
# Tylko wykres — bez printowania pod spodem zbędnej tabeli:
plot_sil <- fviz_silhouette(sil_glass)
# Render wykresu
print(plot_sil + ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
# Tylko wykres — bez printowania pod spodem zbędnej tabeli:
plot_sil <- fviz_silhouette(sil_glass)
# Render wykresu
print(plot_sil$data + ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
# Tylko wykres — bez printowania pod spodem zbędnej tabeli:
plot_sil <- fviz_silhouette(sil_glass)
# Render wykresu
print(plot_sil$plot_env + ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
help(fviz_silhouette
)
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
# Tylko wykres — bez printowania pod spodem zbędnej tabeli:
plot_sil <- fviz_silhouette(sil_glass,print.summary = "False")
# Render wykresu
print(plot_sil$ + ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
Glass.agnes.k6 <- cutree(Glass.agnes.complete, k = 6)
sil_glass <- silhouette(Glass.agnes.k6, dist = Glass.MacNiepodob)
# Tylko wykres — bez printowania pod spodem zbędnej tabeli:
plot_sil <- fviz_silhouette(sil_glass,print.summary = "False")
# Render wykresu
print(plot_sil+ ggtitle("Wykres silhouette dla metody complete linkage (K=6)"))
library(patchwork)
dane_with_clusters <- cbind(dane, PAM_Cluster = pam_clusters, AGNES_Cluster = agnes_clusters)
plot_ri <- ggplot(dane_with_clusters, aes(x = factor(PAM_Cluster), y = RI, fill = factor(PAM_Cluster))) +
geom_boxplot() +
labs(title = "Rozkład RI wg skupień PAM",
x = "Skupienie", y = "RI") +
theme_minimal()
plot_na <- ggplot(dane_with_clusters, aes(x = factor(PAM_Cluster), y = Na, fill = factor(PAM_Cluster))) +
geom_boxplot() +
labs(title = "Rozkład sodu (Na) wg skupień PAM",
x = "Skupienie", y = "Na") +
theme_minimal()
plot_ri + plot_na
library(ggplot2)
library(reshape2)
measures <- slot(internal.validation, "measures")
metrics_array <- as.data.frame.table(measures)
colnames(metrics_array) <- c("Metric", "K", "Method", "Value")
metrics_array$K <- as.numeric(as.character(metrics_array$K))
ggplot(metrics_array, aes(x = K, y = Value, color = Method, group = Method)) +
geom_line(size = 1.2) +
geom_point(size = 2.5) +
facet_wrap(~ Metric, scales = "free_y") +
labs(title = "Porównanie metod klasteryzacji wg metryk wewnętrznych",
x = "Liczba skupień (K)",
y = "Wartość metryki",
color = "Metoda") +
theme_minimal() +
theme(strip.text = element_text(face = "bold", size = 12),
axis.text = element_text(size = 10),
legend.position = "bottom")
library(clValid)
library(cluster)
internal.validation <- clValid(
obj = dane_stand,
nClust = 2:8,
clMethods = c("hierarchical", "pam"),
validation = "internal",
metric = "euclidean",
)
#summary(internal.validation)
knitr::opts_chunk$set(
echo = TRUE,
encoding = "UTF-8",
comment = "",
warning = FALSE,
message = FALSE
)
options(encoding = "UTF-8")
Sys.setlocale("LC_ALL", "Polish")
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(results = "asis")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_chunk$set(fig.pos = "H", out.extra = "", fig.align = "center")
library(mlbench)
library(DataExplorer)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(ipred)
library(randomForest)
library(e1071)
# Rozkład klas
barplot(prop.table(table(dane$Type)), col=4:9, main="Dane Glass - rozkład klas")
grid()
# PCA (wizualizacja danych w 2D)
dane2.pca <- prcomp(dane[, sapply(dane, is.numeric)], center=T, scale=T)
#summary(dane2.pca)
plot(dane2.pca$x[,1:2], col=dane$Type, main="Dane Glass - wykres na bazie PCA", pch=15, cex=0.7)
legend("topright", col=1:9, legend=levels(dane$Type), pch=15, bg="azure2")
# ocena zdolności dyskryminacyjnych poszczególnych zmiennych
plot_boxplot(dane, by="Type")
p <-  ncol(dane) - 1
# różne parametry  (ntree - liczba drzew, mtry - liczba wybieranych losowo cech)
rf.1 <- randomForest(Type~., data=dane, ntree=1, mtry=p, importance=TRUE)
rf.2 <- randomForest(Type~., data=dane, ntree=100, mtry=sqrt(p), importance=TRUE)
pred.labels <- predict(rf.2, newdata=dane, type="class")
real.labels <- dane$Type
(confusion.matrix <- table(pred.labels, real.labels)) # dla zbioru uczącego
pred.probs <- predict(rf.2, newdata=dane, type="prob")
# macierz pomyłek (confusion matrix)  na bazie OOB (Out-Of-Bag), tzn. obserwacji,
# które nie były wybierane w danej replikacji
rf.2$confusion
# Wykres błędu klasyfikacji
plot(rf.2)
legend("topright",
legend = c("Błąd OOB", levels(dane$Type)),
col = 1:(length(levels(dane$Type)) + 1),
lty = 1,
cex = 0.8)
C.range <- 2^((-4):4)
set.seed(123)
gamma.range <- 2^((-8):4)
radial.tune <- tune(svm, train.x=training.set[,c("Mg", "Al")],
train.y=training.set[,"Type"],
kernel="radial",
ranges=list(cost=C.range, gamma=gamma.range))
knitr::opts_chunk$set(
echo = TRUE,
encoding = "UTF-8",
comment = "",
warning = FALSE,
message = FALSE
)
options(encoding = "UTF-8")
Sys.setlocale("LC_ALL", "Polish")
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(results = "asis")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_chunk$set(fig.pos = "H", out.extra = "", fig.align = "center")
library(mlbench)
library(DataExplorer)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(ipred)
library(randomForest)
library(e1071)
data(Glass)
dane <- na.omit(Glass)
## stanaryzacja danych
numeric.features <- sapply(dane, is.numeric)
data.pca <- dane[, numeric.features]
dane_stand <- scale(data.pca)
dane_stand_df <- as.data.frame(dane_stand)
dane_stand_df$Type <- dane$Type
# Rozkład klas
barplot(prop.table(table(dane$Type)), col=4:9, main="Dane Glass - rozkład klas")
grid()
# PCA (wizualizacja danych w 2D)
dane2.pca <- prcomp(dane[, sapply(dane, is.numeric)], center=T, scale=T)
#summary(dane2.pca)
plot(dane2.pca$x[,1:2], col=dane$Type, main="Dane Glass - wykres na bazie PCA", pch=15, cex=0.7)
legend("topright", col=1:9, legend=levels(dane$Type), pch=15, bg="azure2")
# ocena zdolności dyskryminacyjnych poszczególnych zmiennych
plot_boxplot(dane, by="Type")
tree <- rpart(Type~., data=dane) #parametry domyślne
rpart.plot(tree, main="Drzewo klasyfikacyjne - dane Glass", cex=.5)
set.seed(123)
n <- nrow(Glass)
learning.set.index <- sample(1:n, 2/3 * n)
learning.set <- Glass[learning.set.index, ]
test.set <- Glass[-learning.set.index, ]
n.learning <- nrow(learning.set)
n.test <- nrow(test.set)
model <- Type ~ RI + Na + Mg + Al + Si + K + Ca + Ba + Fe
tree.simple <- rpart(model, data = learning.set)
pred.labels.learning <- predict(tree.simple, newdata = learning.set, type = "class")
pred.labels.test <- predict(tree.simple, newdata = test.set, type = "class")
conf.mat.learning <- table(pred.labels.learning, learning.set$Type)
conf.mat.test <- table(pred.labels.test, test.set$Type)
error.rate.learning <- (n.learning - sum(diag(conf.mat.learning))) / n.learning
error.rate.test <- (n.test - sum(diag(conf.mat.test))) / n.test
cat("Błąd klasyfikacji - zbiór uczący:", round(error.rate.learning, 3), "\n")
cat("Błąd klasyfikacji - zbiór testowy:", round(error.rate.test, 3), "\n")
btree <- bagging(Type~., data=dane, nbagg=25, minsplit=1, cp=0)
B.vector <- c(1, 5, 10, 20, 30, 40, 50, 100)
bagging.error.rates <- sapply(B.vector, function(b)  {errorest(Type~., data=dane, model=bagging, nbagg=b, estimator="632plus", est.para=control.errorest(nboot = 20))$error})
plot(B.vector, bagging.error.rates, xlab="B", main="Bagging: error rate vs. B", type="b", col="#A2007B")
grid()
p <-  ncol(dane) - 1
# różne parametry  (ntree - liczba drzew, mtry - liczba wybieranych losowo cech)
rf.1 <- randomForest(Type~., data=dane, ntree=1, mtry=p, importance=TRUE)
rf.2 <- randomForest(Type~., data=dane, ntree=100, mtry=sqrt(p), importance=TRUE)
pred.labels <- predict(rf.2, newdata=dane, type="class")
real.labels <- dane$Type
#(confusion.matrix <- table(pred.labels, real.labels)) # dla zbioru uczącego
pred.probs <- predict(rf.2, newdata=dane, type="prob")
# macierz pomyłek (confusion matrix)  na bazie OOB (Out-Of-Bag), tzn. obserwacji,
# które nie były wybierane w danej replikacji
rf.2$confusion
# Wykres błędu klasyfikacji
plot(rf.2)
legend("topright",
legend = c("Błąd OOB", levels(dane$Type)),
col = 1:(length(levels(dane$Type)) + 1),
lty = 1,
cex = 0.8)
# Ranking ważności cech
varImpPlot(rf.2,
main = "Istotność zmiennych",
cex.main = 0.7)
set.seed(123)
data(Glass)
dane <- na.omit(Glass)
## stanaryzacja danych
numeric.features <- sapply(dane, is.numeric)
data.pca <- dane[, numeric.features]
dane_stand <- scale(data.pca)
dane_stand_df <- as.data.frame(dane_stand)
dane_stand_df$Type <- dane$Type
n <- nrow(dane)
learn.ind    <- sample(1:n, 2/3*n)
training.set <- dane_stand_df[learn.ind, ]
test.set     <- dane_stand_df[-learn.ind, ]
par(mfrow = c(1, 2))
## c=0.1
svm.linear.c0.1 <- svm(Type ~ Mg + Al, data=training.set, kernel="linear", cost=0.1)
##summary(svm.linear.C0.1)
plot(svm.linear.c0.1, data=training.set, Mg~Al, svSymbol=16, grid=100)
real.labels <- test.set$Type
n.test <- length(real.labels)
pred.svm.lin.c0.1 <- predict(svm.linear.c0.1, newdata=test.set)
acc.svm.lin.c0.1 <- sum(diag(table(pred.svm.lin.c0.1, real.labels))) / n.test
cat("Skuteczność klasyfikacji:", round(acc.svm.lin.c0.1 , 3), "\n")
## c=1
svm.linear.c0.1 <- svm(Type ~ Mg + Al, data=training.set, kernel="linear", cost=1)
##summary(svm.linear.C0.1)
plot(svm.linear.c0.1, data=training.set, Mg~Al, svSymbol=16, grid=100)
real.labels <- test.set$Type
n.test <- length(real.labels)
pred.svm.lin.c0.1 <- predict(svm.linear.c0.1, newdata=test.set)
acc.svm.lin.c0.1 <- sum(diag(table(pred.svm.lin.c0.1, real.labels))) / n.test
cat("Skuteczność klasyfikacji:", round(acc.svm.lin.c0.1 , 3), "\n")
## c=10
svm.linear.c0.1 <- svm(Type ~ Mg + Al, data=training.set, kernel="linear", cost=10)
##summary(svm.linear.C0.1)
plot(svm.linear.c0.1, data=training.set, Mg~Al, svSymbol=16, grid=100)
real.labels <- test.set$Type
n.test <- length(real.labels)
pred.svm.lin.c0.1 <- predict(svm.linear.c0.1, newdata=test.set)
acc.svm.lin.c0.1 <- sum(diag(table(pred.svm.lin.c0.1, real.labels))) / n.test
cat("Skuteczność klasyfikacji:", round(acc.svm.lin.c0.1 , 3), "\n")
## c=0.1
svm.linear.c0.1 <- svm(Type ~ Mg + Al, data=training.set, kernel="radial",cost =0.1)
##summary(svm.linear.C0.1)
plot(svm.linear.c0.1, data=training.set, Mg~Al, svSymbol=16, grid=100)
real.labels <- test.set$Type
n.test <- length(real.labels)
pred.svm.lin.c0.1 <- predict(svm.linear.c0.1, newdata=test.set)
acc.svm.lin.c0.1 <- sum(diag(table(pred.svm.lin.c0.1, real.labels))) / n.test
cat("Skuteczność klasyfikacji:", round(acc.svm.lin.c0.1 , 3), "\n")
## c=1
svm.linear.c0.1 <- svm(Type ~ Mg + Al, data=training.set, kernel="radial", cost=1)
##summary(svm.linear.C0.1)
plot(svm.linear.c0.1, data=training.set, Mg~Al, svSymbol=16, grid=100)
real.labels <- test.set$Type
n.test <- length(real.labels)
pred.svm.lin.c0.1 <- predict(svm.linear.c0.1, newdata=test.set)
acc.svm.lin.c0.1 <- sum(diag(table(pred.svm.lin.c0.1, real.labels))) / n.test
cat("Skuteczność klasyfikacji:", round(acc.svm.lin.c0.1 , 3), "\n")
## c=10
svm.linear.c0.1 <- svm(Type ~ Mg + Al, data=training.set, kernel="radial", cost=10)
##summary(svm.linear.C0.1)
plot(svm.linear.c0.1, data=training.set, Mg~Al, svSymbol=16, grid=100)
real.labels <- test.set$Type
n.test <- length(real.labels)
pred.svm.lin.c0.1 <- predict(svm.linear.c0.1, newdata=test.set)
acc.svm.lin.c0.1 <- sum(diag(table(pred.svm.lin.c0.1, real.labels))) / n.test
cat("Skuteczność klasyfikacji:", round(acc.svm.lin.c0.1 , 3), "\n")
C.range <- 2^((-4):4)
set.seed(123)
gamma.range <- 2^((-8):4)
radial.tune <- tune(svm, train.x=training.set[,c("Mg", "Al")],
train.y=training.set[,"Type"],
kernel="radial",
ranges=list(cost=C.range, gamma=gamma.range))
print(radial.tune)
plot(radial.tune, transform.x=log, transform.y=log, color.palette = topo.colors)
plot(radial.tune, type="perspective")
C.best <- radial.tune$best.parameters[["cost"]]
gamma.best <- radial.tune$best.parameters[["gamma"]]
svm.radial.tuned <- svm(Type~Mg+Al,
data=training.set, kernel="radial",
cost=C.best, gamma=gamma.best)
##summary(svm.radial.tuned)
pred.svm.radial.tuned <- predict(svm.radial.tuned, newdata=test.set)
acc.svm.radial <- sum(diag(table(pred.svm.radial.tuned, real.labels)))/n.test
cat("Skuteczność klasyfikacji:", round(acc.svm.radial, 3), "\n")
C.range <- 2^((-4):4)
set.seed(123)
gamma.range <- 2^((-8):4)
radial.tune <- tune(svm, train.x=training.set[,c("Mg", "Al")],
train.y=training.set[,"Type"],
kernel="radial",
ranges=list(cost=C.range, gamma=gamma.range))
#print(radial.tune)
plot(radial.tune, transform.x=log, transform.y=log, color.palette = topo.colors)
plot(radial.tune, type="perspective")
C.best <- radial.tune$best.parameters[["cost"]]
gamma.best <- radial.tune$best.parameters[["gamma"]]
svm.radial.tuned <- svm(Type~Mg+Al,
data=training.set, kernel="radial",
cost=C.best, gamma=gamma.best)
##summary(svm.radial.tuned)
pred.svm.radial.tuned <- predict(svm.radial.tuned, newdata=test.set)
acc.svm.radial <- sum(diag(table(pred.svm.radial.tuned, real.labels)))/n.test
cat("Skuteczność klasyfikacji:", round(acc.svm.radial, 3), "\n")
Help(plot_boxplot)
par(mfrow = (3,3))
par(mfrow = c(3, 3))
plot_boxplot(dane, by="Type")
par(mfrow = c(3, 3), mar = c(4, 4, 2, 1))  # Adjust margins
plot_boxplot(dane, by="Type")
knitr::opts_chunk$set(
echo = TRUE,
encoding = "UTF-8",
comment = "",
warning = FALSE,
message = FALSE
)
options(encoding = "UTF-8")
Sys.setlocale("LC_ALL", "Polish")
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(results = "asis")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_chunk$set(fig.pos = "H", out.extra = "", fig.align = "center")
library(mlbench)
library(DataExplorer)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(ipred)
library(randomForest)
library(e1071)
p <-  ncol(dane) - 1
# różne parametry  (ntree - liczba drzew, mtry - liczba wybieranych losowo cech)
rf.1 <- randomForest(Type~., data=dane, ntree=1, mtry=p, importance=TRUE)
rf.2 <- randomForest(Type~., data=dane, ntree=100, mtry=sqrt(p), importance=TRUE)
pred.labels <- predict(rf.2, newdata=dane, type="class")
real.labels <- dane$Type
#(confusion.matrix <- table(pred.labels, real.labels)) # dla zbioru uczącego
pred.probs <- predict(rf.2, newdata=dane, type="prob")
# macierz pomyłek (confusion matrix)  na bazie OOB (Out-Of-Bag), tzn. obserwacji,
# które nie były wybierane w danej replikacji
rf.2$confusion
# Wykres błędu klasyfikacji
plot(rf.2)
legend("topright",
legend = c("Błąd OOB", levels(dane$Type)),
col = 1:(length(levels(dane$Type)) + 1),
lty = 1,
cex = 0.8)
p <-  ncol(dane) - 1
# różne parametry  (ntree - liczba drzew, mtry - liczba wybieranych losowo cech)
rf.1 <- randomForest(Type~., data=dane, ntree=1, mtry=p, importance=TRUE)
rf.2 <- randomForest(Type~., data=dane, ntree=100, mtry=sqrt(p), importance=TRUE)
pred.labels <- predict(rf.2, newdata=dane, type="class")
real.labels <- dane$Type
#(confusion.matrix <- table(pred.labels, real.labels)) # dla zbioru uczącego
pred.probs <- predict(rf.2, newdata=dane, type="prob")
# macierz pomyłek (confusion matrix)  na bazie OOB (Out-Of-Bag), tzn. obserwacji,
# które nie były wybierane w danej replikacji
#rf.2$confusion
# Wykres błędu klasyfikacji
plot(rf.2)
legend("topright",
legend = c("Błąd OOB", levels(dane$Type)),
col = 1:(length(levels(dane$Type)) + 1),
lty = 1,
cex = 0.8)
